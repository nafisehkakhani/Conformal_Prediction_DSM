{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformalized quantile regression (CQR): Real data experiment\n",
    "\n",
    "In this tutorial we will load a real dataset and construct prediction intervals using CQR [1].\n",
    "\n",
    "[1] Yaniv Romano, Evan Patterson, and Emmanuel J. Candes, “Conformalized quantile regression.” 2019.\n",
    "\n",
    "## Prediction intervals\n",
    "\n",
    "Suppose we are given $ n $ training samples $ \\{(X_i, Y_i)\\}_{i=1}^n$ and we must now predict the unknown value of $Y_{n+1}$ at a test point $X_{n+1}$. We assume that all the samples $ \\{(X_i,Y_i)\\}_{i=1}^{n+1} $ are drawn exchangeably$-$for instance, they may be drawn i.i.d.$-$from an arbitrary joint distribution $P_{XY}$ over the feature vectors $ X\\in \\mathbb{R}^p $ and response variables $ Y\\in \\mathbb{R} $. We aim to construct a marginal distribution-free prediction interval $C(X_{n+1}) \\subseteq \\mathbb{R}$ that is likely to contain the unknown response $Y_{n+1} $. That is, given a desired miscoverage rate $ \\alpha $, we ask that\n",
    "$$ \\mathbb{P}\\{Y_{n+1} \\in C(X_{n+1})\\} \\geq 1-\\alpha $$\n",
    "for any joint distribution $ P_{XY} $ and any sample size $n$. The probability in this statement is marginal, being taken over all the samples $ \\{(X_i, Y_i)\\}_{i=1}^{n+1} $.\n",
    "\n",
    "To accomplish this, we build on the method of split conformal prediction. We first split the training data into two disjoint subsets, a proper training set and a calibration set. We fit two quantile regressors on the proper training set to obtain initial estimates of the lower and upper bounds of the prediction interval. Then, using the calibration set, we conformalize and, if necessary, correct this prediction interval. Unlike the original interval, the conformalized prediction interval is guaranteed to satisfy the coverage requirement regardless of the choice or accuracy of the quantile regression estimator.\n",
    "\n",
    "\n",
    "\n",
    "## A case study\n",
    "\n",
    "We start by importing several libraries, loading the real dataset and standardize its features and response. We set the target miscoverage rate $\\alpha$ to 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('D:\\Conformalized_Quantile_Regression\\LUCAS_2015_features_V3.csv')\n",
    "df = pd.read_csv('C:\\\\Users\\\\nkakhani\\\\_CP_DSM\\\\Conformal_Prediction_DSM\\\\LUCAS_2015_features_V3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to numeric, omitting non-double values\n",
    "df['OC'] = pd.to_numeric(df['OC'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (non-double values)\n",
    "df.dropna(subset=['OC'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SR_B3_1</th>\n",
       "      <th>SR_B3_2</th>\n",
       "      <th>SR_B3_3</th>\n",
       "      <th>SR_B3_4</th>\n",
       "      <th>SR_B4_1</th>\n",
       "      <th>SR_B4_2</th>\n",
       "      <th>SR_B4_3</th>\n",
       "      <th>SR_B4_4</th>\n",
       "      <th>SR_B5_1</th>\n",
       "      <th>SR_B5_2</th>\n",
       "      <th>...</th>\n",
       "      <th>average_7</th>\n",
       "      <th>average_8</th>\n",
       "      <th>average_9</th>\n",
       "      <th>average_10</th>\n",
       "      <th>average_11</th>\n",
       "      <th>average_12</th>\n",
       "      <th>average_13</th>\n",
       "      <th>average_14</th>\n",
       "      <th>OC</th>\n",
       "      <th>point_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.053137</td>\n",
       "      <td>0.053137</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>...</td>\n",
       "      <td>532.638051</td>\n",
       "      <td>498.618622</td>\n",
       "      <td>502.788853</td>\n",
       "      <td>704.755629</td>\n",
       "      <td>402.898849</td>\n",
       "      <td>469.73883</td>\n",
       "      <td>617.714286</td>\n",
       "      <td>396.468312</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26581768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037099</td>\n",
       "      <td>0.036429</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.035307</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.033989</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>0.179388</td>\n",
       "      <td>0.175070</td>\n",
       "      <td>...</td>\n",
       "      <td>512.001688</td>\n",
       "      <td>489.344548</td>\n",
       "      <td>512.317155</td>\n",
       "      <td>678.640244</td>\n",
       "      <td>404.306692</td>\n",
       "      <td>481.17483</td>\n",
       "      <td>593.808163</td>\n",
       "      <td>402.009979</td>\n",
       "      <td>21.9</td>\n",
       "      <td>26581792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072997</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.071491</td>\n",
       "      <td>0.072426</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>0.065369</td>\n",
       "      <td>0.060863</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.347341</td>\n",
       "      <td>0.352894</td>\n",
       "      <td>...</td>\n",
       "      <td>519.103506</td>\n",
       "      <td>509.807511</td>\n",
       "      <td>457.939797</td>\n",
       "      <td>668.159475</td>\n",
       "      <td>448.914535</td>\n",
       "      <td>505.68683</td>\n",
       "      <td>625.240816</td>\n",
       "      <td>395.747479</td>\n",
       "      <td>18.4</td>\n",
       "      <td>26581954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026582</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>0.026356</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.022784</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.189531</td>\n",
       "      <td>0.186850</td>\n",
       "      <td>...</td>\n",
       "      <td>520.863506</td>\n",
       "      <td>495.344548</td>\n",
       "      <td>490.902061</td>\n",
       "      <td>686.594090</td>\n",
       "      <td>401.957672</td>\n",
       "      <td>480.77883</td>\n",
       "      <td>613.922449</td>\n",
       "      <td>398.876646</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26601784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051178</td>\n",
       "      <td>0.051178</td>\n",
       "      <td>0.047617</td>\n",
       "      <td>0.047617</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.358772</td>\n",
       "      <td>0.358772</td>\n",
       "      <td>...</td>\n",
       "      <td>497.983506</td>\n",
       "      <td>490.677881</td>\n",
       "      <td>449.362438</td>\n",
       "      <td>634.390244</td>\n",
       "      <td>435.981202</td>\n",
       "      <td>489.49483</td>\n",
       "      <td>594.461224</td>\n",
       "      <td>388.101646</td>\n",
       "      <td>25.2</td>\n",
       "      <td>26601978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21853</th>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.073323</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.073102</td>\n",
       "      <td>0.110663</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.111120</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.185107</td>\n",
       "      <td>0.184097</td>\n",
       "      <td>...</td>\n",
       "      <td>644.881688</td>\n",
       "      <td>600.148252</td>\n",
       "      <td>477.336023</td>\n",
       "      <td>746.617167</td>\n",
       "      <td>403.169437</td>\n",
       "      <td>416.83883</td>\n",
       "      <td>690.440816</td>\n",
       "      <td>344.026646</td>\n",
       "      <td>8.4</td>\n",
       "      <td>64881666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21854</th>\n",
       "      <td>0.085582</td>\n",
       "      <td>0.085886</td>\n",
       "      <td>0.086188</td>\n",
       "      <td>0.086588</td>\n",
       "      <td>0.071660</td>\n",
       "      <td>0.072601</td>\n",
       "      <td>0.071150</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>0.402217</td>\n",
       "      <td>0.400485</td>\n",
       "      <td>...</td>\n",
       "      <td>636.685324</td>\n",
       "      <td>590.337140</td>\n",
       "      <td>476.524702</td>\n",
       "      <td>736.836398</td>\n",
       "      <td>395.087084</td>\n",
       "      <td>410.73883</td>\n",
       "      <td>680.738776</td>\n",
       "      <td>341.705812</td>\n",
       "      <td>10.8</td>\n",
       "      <td>64901668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21855</th>\n",
       "      <td>0.084536</td>\n",
       "      <td>0.087933</td>\n",
       "      <td>0.081859</td>\n",
       "      <td>0.085314</td>\n",
       "      <td>0.071451</td>\n",
       "      <td>0.077255</td>\n",
       "      <td>0.066329</td>\n",
       "      <td>0.072025</td>\n",
       "      <td>0.391538</td>\n",
       "      <td>0.383660</td>\n",
       "      <td>...</td>\n",
       "      <td>636.685324</td>\n",
       "      <td>590.337140</td>\n",
       "      <td>476.524702</td>\n",
       "      <td>736.836398</td>\n",
       "      <td>395.087084</td>\n",
       "      <td>410.73883</td>\n",
       "      <td>680.738776</td>\n",
       "      <td>341.705812</td>\n",
       "      <td>6.7</td>\n",
       "      <td>64901672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21856</th>\n",
       "      <td>0.087019</td>\n",
       "      <td>0.087398</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.087840</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>0.102079</td>\n",
       "      <td>0.099499</td>\n",
       "      <td>0.102728</td>\n",
       "      <td>0.286553</td>\n",
       "      <td>0.283027</td>\n",
       "      <td>...</td>\n",
       "      <td>648.761688</td>\n",
       "      <td>602.381585</td>\n",
       "      <td>490.090740</td>\n",
       "      <td>754.286398</td>\n",
       "      <td>404.941986</td>\n",
       "      <td>417.09883</td>\n",
       "      <td>695.187755</td>\n",
       "      <td>351.993312</td>\n",
       "      <td>5.7</td>\n",
       "      <td>64961676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21857</th>\n",
       "      <td>0.077897</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.077860</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.074107</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.075339</td>\n",
       "      <td>0.265865</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>...</td>\n",
       "      <td>645.212597</td>\n",
       "      <td>597.070474</td>\n",
       "      <td>487.453004</td>\n",
       "      <td>744.874859</td>\n",
       "      <td>399.334143</td>\n",
       "      <td>417.05883</td>\n",
       "      <td>688.228571</td>\n",
       "      <td>348.451646</td>\n",
       "      <td>18.1</td>\n",
       "      <td>64981672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21858 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SR_B3_1   SR_B3_2   SR_B3_3   SR_B3_4   SR_B4_1   SR_B4_2   SR_B4_3  \\\n",
       "0      0.065401  0.065401  0.063259  0.063259  0.058280  0.058280  0.053137   \n",
       "1      0.037099  0.036429  0.035900  0.035307  0.034250  0.033989  0.032527   \n",
       "2      0.072997  0.075304  0.071491  0.072426  0.063204  0.065369  0.060863   \n",
       "3      0.026582  0.026362  0.026356  0.026522  0.022784  0.022857  0.022424   \n",
       "4      0.051178  0.051178  0.047617  0.047617  0.031081  0.031081  0.028169   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21853  0.073742  0.073323  0.073456  0.073102  0.110663  0.109788  0.111120   \n",
       "21854  0.085582  0.085886  0.086188  0.086588  0.071660  0.072601  0.071150   \n",
       "21855  0.084536  0.087933  0.081859  0.085314  0.071451  0.077255  0.066329   \n",
       "21856  0.087019  0.087398  0.086868  0.087840  0.099747  0.102079  0.099499   \n",
       "21857  0.077897  0.077503  0.078162  0.077860  0.074805  0.074107  0.075763   \n",
       "\n",
       "        SR_B4_4   SR_B5_1   SR_B5_2  ...   average_7   average_8   average_9  \\\n",
       "0      0.053137  0.341738  0.341738  ...  532.638051  498.618622  502.788853   \n",
       "1      0.032337  0.179388  0.175070  ...  512.001688  489.344548  512.317155   \n",
       "2      0.061394  0.347341  0.352894  ...  519.103506  509.807511  457.939797   \n",
       "3      0.022880  0.189531  0.186850  ...  520.863506  495.344548  490.902061   \n",
       "4      0.028169  0.358772  0.358772  ...  497.983506  490.677881  449.362438   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "21853  0.110324  0.185107  0.184097  ...  644.881688  600.148252  477.336023   \n",
       "21854  0.072110  0.402217  0.400485  ...  636.685324  590.337140  476.524702   \n",
       "21855  0.072025  0.391538  0.383660  ...  636.685324  590.337140  476.524702   \n",
       "21856  0.102728  0.286553  0.283027  ...  648.761688  602.381585  490.090740   \n",
       "21857  0.075339  0.265865  0.267717  ...  645.212597  597.070474  487.453004   \n",
       "\n",
       "       average_10  average_11  average_12  average_13  average_14    OC  \\\n",
       "0      704.755629  402.898849   469.73883  617.714286  396.468312  24.6   \n",
       "1      678.640244  404.306692   481.17483  593.808163  402.009979  21.9   \n",
       "2      668.159475  448.914535   505.68683  625.240816  395.747479  18.4   \n",
       "3      686.594090  401.957672   480.77883  613.922449  398.876646  48.0   \n",
       "4      634.390244  435.981202   489.49483  594.461224  388.101646  25.2   \n",
       "...           ...         ...         ...         ...         ...   ...   \n",
       "21853  746.617167  403.169437   416.83883  690.440816  344.026646   8.4   \n",
       "21854  736.836398  395.087084   410.73883  680.738776  341.705812  10.8   \n",
       "21855  736.836398  395.087084   410.73883  680.738776  341.705812   6.7   \n",
       "21856  754.286398  404.941986   417.09883  695.187755  351.993312   5.7   \n",
       "21857  744.874859  399.334143   417.05883  688.228571  348.451646  18.1   \n",
       "\n",
       "       point_id  \n",
       "0      26581768  \n",
       "1      26581792  \n",
       "2      26581954  \n",
       "3      26601784  \n",
       "4      26601978  \n",
       "...         ...  \n",
       "21853  64881666  \n",
       "21854  64901668  \n",
       "21855  64901672  \n",
       "21856  64961676  \n",
       "21857  64981672  \n",
       "\n",
       "[21858 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[1:10000,:-2]\n",
    "y = df.iloc[1:10000,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: train set (n=7999, p=72) ; test set (n=2000, p=72)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "seed = 1\n",
    "\n",
    "random_state_train_test = seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# desired miscoverage error\n",
    "alpha = 0.1\n",
    "\n",
    "# desired quanitile levels\n",
    "quantiles = [0.05, 0.95]\n",
    "\n",
    "# used to determine the size of test set\n",
    "test_ratio = 0.2\n",
    "\n",
    "# # name of dataset\n",
    "# dataset_base_path = \"./datasets/\"\n",
    "# dataset_name = \"community\"\n",
    "\n",
    "# # load the dataset\n",
    "# X, y = datasets.GetDataset(dataset_name, dataset_base_path)\n",
    "\n",
    "# divide the dataset into test and train based on the test_ratio parameter\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_ratio,\n",
    "                                                    random_state=random_state_train_test)\n",
    "                                                    \n",
    "#let us keep the indices for test samples for furthure mapping\n",
    "keep_inds = x_test.index.tolist()\n",
    "point_id = df.loc[keep_inds, 'point_id']\n",
    "\n",
    "# reshape the data\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "# compute input dimensions\n",
    "n_train = x_train.shape[0]\n",
    "in_shape = x_train.shape[1]\n",
    "\n",
    "# display basic information\n",
    "# print(\"Dataset: %s\" % (dataset_name))\n",
    "print(\"Dimensions: train set (n=%d, p=%d) ; test set (n=%d, p=%d)\" % \n",
    "      (x_train.shape[0], x_train.shape[1], x_test.shape[0], x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "We begin by splitting the data into a proper training set and a calibration set. Recall that the main idea is to fit a regression model on the proper training samples, then use the residuals on a held-out validation set to quantify the uncertainty in future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the data into proper training set and calibration set\n",
    "idx = np.random.permutation(n_train)\n",
    "split_point = int(np.floor(n_train * 0.9))\n",
    "# idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]\n",
    "\n",
    "# Split the indices into training and calibration sets\n",
    "idx_train, idx_cal = idx[:split_point], idx[split_point:]\n",
    "\n",
    "# zero mean and unit variance scaling \n",
    "scalerX = StandardScaler()\n",
    "scalerX = scalerX.fit(x_train[idx_train])\n",
    "\n",
    "# scale\n",
    "x_train = scalerX.transform(x_train)\n",
    "x_test = scalerX.transform(x_test)\n",
    "\n",
    "# scale the labels by dividing each by the mean absolute response\n",
    "mean_y_train = np.mean(np.abs(y_train[idx_train]))\n",
    "# y_train = np.squeeze(y_train)/mean_y_train\n",
    "\n",
    "#using log transformation to see whether the results are improved\n",
    "y_train = np.log(np.squeeze(y_train))\n",
    "y_test = np.log(np.squeeze(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cqr import helper\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# from math import sqrt\n",
    "\n",
    "# from sklearn.model_selection import KFold, GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "\n",
    "# # Create your dataset (x_train, y_train, x_test, y_test)\n",
    "\n",
    "# # Define the parameter grid for hyperparameter tuning\n",
    "# param_grid = {\n",
    "#     \"min_samples_leaf\": [1, 2, 5],\n",
    "#     \"n_estimators\": [100, 500, 1000],\n",
    "#     \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "# }\n",
    "\n",
    "# # Perform nested cross-validation (e.g., 5-fold outer, 3-fold inner)\n",
    "# outer_cv = KFold(n_splits=5, shuffle=True, random_state=422)\n",
    "# inner_cv = KFold(n_splits=3, shuffle=True, random_state=422)\n",
    "\n",
    "# # Initialize lists to store results\n",
    "# mean_mse_scores = []\n",
    "# std_mse_scores = []\n",
    "\n",
    "# for train_index, test_index in outer_cv.split(x_train):\n",
    "#     x_train_outer, x_test_outer = x_train[train_index], x_train[test_index]\n",
    "#     y_train_outer, y_test_outer = y_train[train_index], y_train[test_index]\n",
    "\n",
    "#     # Initialize the inner loop results\n",
    "#     inner_residual_matrix = []\n",
    "\n",
    "#     for inner_train_index, inner_test_index in inner_cv.split(x_train_outer):\n",
    "#         x_train_inner, x_val_inner = x_train_outer[inner_train_index], x_train_outer[inner_test_index]\n",
    "#         y_train_inner, y_val_inner = y_train_outer[inner_train_index], y_train_outer[inner_test_index]\n",
    "\n",
    "#         # Initialize the inner GridSearchCV for hyperparameter tuning\n",
    "#         grid_search = GridSearchCV(\n",
    "#             estimator=RandomForestRegressor(random_state=422),\n",
    "#             param_grid=param_grid,\n",
    "#             scoring='neg_mean_squared_error',\n",
    "#             cv=inner_cv,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "\n",
    "#         # Fit the GridSearchCV to the inner training data\n",
    "#         grid_search.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "#         # Get the best hyperparameters from the inner loop\n",
    "#         best_params = grid_search.best_params_\n",
    "\n",
    "#         # Create a RandomForestRegressor with the best hyperparameters\n",
    "#         rf_model = RandomForestRegressor(\n",
    "#             random_state=422,\n",
    "#             min_samples_leaf=best_params['min_samples_leaf'],\n",
    "#             n_estimators=best_params['n_estimators'],\n",
    "#             max_features=best_params['max_features']\n",
    "#         )\n",
    "\n",
    "#         # Fit the model to the inner training data\n",
    "#         rf_model.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "#         # Make predictions on the inner validation data\n",
    "#         predictions = rf_model.predict(x_val_inner)\n",
    "\n",
    "#         # Calculate residuals for the inner fold and store them in the inner_residual_matrix\n",
    "#         residuals = y_val_inner - predictions\n",
    "#         inner_residual_matrix.append(residuals)\n",
    "\n",
    "#     # Calculate the mean and standard deviation of MSE scores for the inner loop results\n",
    "#     inner_mse_scores = [mean_squared_error(y_val_inner, predictions) for predictions in rf_model.staged_predict(x_test_outer)]\n",
    "#     mean_inner_mse = np.mean(inner_mse_scores)\n",
    "#     std_inner_mse = np.std(inner_mse_scores)\n",
    "\n",
    "#     # Append the mean and standard deviation of inner MSE to the lists of outer loop results\n",
    "#     mean_mse_scores.append(mean_inner_mse)\n",
    "#     std_mse_scores.append(std_inner_mse)\n",
    "\n",
    "#     # Combine residuals from the inner loop into a single matrix\n",
    "#     inner_residual_matrix = np.vstack(inner_residual_matrix)\n",
    "    \n",
    "#     # Now, you can fit a quantile regression model using the inner_residual_matrix\n",
    "#     # For example, using the statsmodels library:\n",
    "#     import statsmodels.api as sm\n",
    "\n",
    "#     quantile_model = sm.QuantReg(y_train, x_train)\n",
    "#     quantile_results = quantile_model.fit(q=0.5)  # Fit the model for the median (you can choose other quantiles)\n",
    "#     print(quantile_results.summary())\n",
    "\n",
    "# # Calculate the mean and standard deviation of MSE scores from the outer loop\n",
    "# mean_mse = np.mean(mean_mse_scores)\n",
    "# std_mse = np.mean(std_mse_scores)\n",
    "\n",
    "# print(f\"Mean MSE: {mean_mse}\")\n",
    "# print(f\"Standard Deviation of MSE: {std_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Make predictions on the test data\n",
    "# test_predictions = rf_model.predict(x_test)\n",
    "\n",
    "# # Calculate R-squared (R²) for test samples\n",
    "# r2_test = r2_score(y_test, test_predictions)\n",
    "\n",
    "# # Calculate Root Mean Squared Error (RMSE) for test samples\n",
    "# rmse_test = sqrt(mean_squared_error(y_test, test_predictions))\n",
    "\n",
    "# print(f\"R² for Test Samples: {r2_test}\")\n",
    "# print(f\"RMSE for Test Samples: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the number of bootstrap iterations\n",
    "# num_bootstraps = 10  # Adjust as needed\n",
    "\n",
    "# # Initialize an empty array to store bootstrap results\n",
    "# bootstrap_results = np.zeros((num_bootstraps, len(y_test)))\n",
    "\n",
    "# for i in range(num_bootstraps):\n",
    "#     # Generate random indices with replacement\n",
    "#     indices = np.random.choice(len(y_train), len(y_train), replace=True)\n",
    "    \n",
    "#     # Select a bootstrap sample\n",
    "#     bootstrap_x = x_train[indices]\n",
    "#     bootstrap_y = y_train[indices]\n",
    "    \n",
    "#     # Train your model on the bootstrap sample (e.g., rf_model.fit(bootstrap_x, bootstrap_y))\n",
    "#     rf_model.fit(bootstrap_x, bootstrap_y)\n",
    "    \n",
    "#     # Make predictions on the entire dataset\n",
    "#     bootstrap_results[i] = rf_model.predict(x_test)\n",
    "\n",
    "# # Calculate statistics from the bootstrap results (e.g., confidence intervals)\n",
    "# mean_predictions = np.mean(bootstrap_results, axis=0)\n",
    "# lower_bound_rf = np.percentile(bootstrap_results, 2.5, axis=0)\n",
    "# upper_bound_rf = np.percentile(bootstrap_results, 97.5, axis=0)\n",
    "\n",
    "# # Calculate metrics or uncertainty measures using the bootstrap results\n",
    "# mse_bootstrap = mean_squared_error(y_test, mean_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_lower_rf = np.exp(lower_bound_rf) \n",
    "# y_upper_rf = np.exp(upper_bound_rf) \n",
    "# y_test_rf = np.exp(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cqr import helper\n",
    "# # compute and print average coverage and average length\n",
    "# coverage_RF, length_RF = helper.compute_coverage(y_test_rf,\n",
    "#                                                  y_lower_rf,\n",
    "#                                                  y_upper_rf,\n",
    "#                                                  alpha,\n",
    "#                                                  \"Random Forests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame with the desired columns\n",
    "# df_oc_rf = pd.DataFrame({\n",
    "#     'lower_oc': y_lower_rf,\n",
    "#     'upper_oc': y_upper_rf,\n",
    "#     'predicted_oc': (y_upper_rf + y_lower_rf)/2,\n",
    "#     'standard_uncertainty': (y_upper_rf - y_lower_rf) / np.mean((y_upper_rf + y_lower_rf)),\n",
    "#     'test_oc': y_test_rf,\n",
    "#     'Point_ID': point_id\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_oc_rf.to_csv('D:\\Conformalized_Quantile_Regression\\LUCAS_2015_rf.csv', index = False)  # Set index=False to exclude row indices in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQR random forests\n",
    "\n",
    "Given these two subsets, we now turn to conformalize the initial prediction interval constructed by quantile random forests [2]. Below, we set the hyper-parameters of the CQR random forests method.\n",
    "\n",
    "[2] Meinshausen Nicolai. \"Quantile regression forests.\" Journal of Machine Learning Research 7, no. Jun (2006): 983-999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Quantile random forests parameters\n",
    "# (See QuantileForestRegressorAdapter class in helper.py)\n",
    "#########################################################\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 1000\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = x_train.shape[1]\n",
    "\n",
    "# target quantile levels\n",
    "quantiles_forest = [quantiles[0]*100, quantiles[1]*100]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 0.85\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = 1\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric nonconformity score \n",
    "\n",
    "In the following cell we run the entire CQR procudure. The class `QuantileForestRegressorAdapter` defines the underlying estimator. The class `RegressorNc` defines the CQR objecct, which uses `QuantileRegErrFunc` as the nonconformity score. The function `run_icp` fits the regression function to the proper training set, corrects (if required) the initial estimate of the prediction interval using the calibration set, and returns the conformal band. Lastly, we compute the average coverage and length on future test data using `compute_coverage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "c:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.nc import QuantileRegErrFunc\n",
    "\n",
    "# define the QRF's parameters \n",
    "params_qforest = dict()\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"max_features\"] = max_features\n",
    "params_qforest[\"CV\"] = cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"] = cv_test_ratio\n",
    "params_qforest[\"random_state\"] = cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF: Percentage in the range (expecting 90.00): 33.100000\n",
      "RF: Average length: 0.486211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 20\n",
    "\n",
    "# n_resample = 1000\n",
    "\n",
    "# model_rf = RandomForestRegressor(n_estimators = n_estimators, min_samples_leaf = min_samples_leaf, random_state=0)\n",
    "\n",
    "# Initialize lists to store predictions for each bootstrap sample\n",
    "bootstrap_predictions = []\n",
    "\n",
    "# # Perform bootstrapping and calculate predictions as before\n",
    "# for _ in range(n_bootstraps):\n",
    "#     X_boot, y_boot = resample(x_train, y_train, n_samples = len(y_train) - n_resample , random_state=np.random.randint(0, 100))\n",
    "#     model_rf.fit(X_boot, y_boot)\n",
    "#     y_pred = model_rf.predict(x_test)\n",
    "#     bootstrap_predictions.append(y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Standard bootstrap: Omit around 36.8% of samples in each iteration\n",
    "sample_size = int(0.632 * x_train.shape[0])  # 63.2% of training samples\n",
    "    \n",
    "for _ in range(n_bootstraps):\n",
    "        # Create a new bootstrap sample by randomly selecting samples with replacement\n",
    "        bootstrap_indices = np.random.choice(x_train.shape[0], size=sample_size, replace=True)\n",
    "        X_bootstrap = x_train[bootstrap_indices]\n",
    "        y_bootstrap = y_train[bootstrap_indices]\n",
    "        \n",
    "        # Create and train a new model on the bootstrap sample\n",
    "        bootstrap_model = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, random_state=0)\n",
    "        bootstrap_model.fit(X_bootstrap, y_bootstrap)\n",
    "        \n",
    "        # Make predictions on the test data using the bootstrap model\n",
    "        y_bootstrap_pred = bootstrap_model.predict(x_test)\n",
    "        \n",
    "        # Append the predictions to the list\n",
    "        bootstrap_predictions.append(y_bootstrap_pred)\n",
    "    \n",
    "\n",
    "y_upper = np.max(bootstrap_predictions, axis=0)\n",
    "y_lower = np.min(bootstrap_predictions, axis=0)\n",
    "\n",
    "coverage_forest, length_forest = helper.compute_coverage(y_test,y_lower,y_upper,alpha,\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the desired columns\n",
    "df_oc = pd.DataFrame({\n",
    "    'BootRF_lower_oc': np.exp(y_lower),\n",
    "    'BootRF_upper_oc': np.exp(y_upper),\n",
    "    'BootRF_predicted_oc': (np.exp(y_upper) + np.exp(y_lower))/2,\n",
    "    'BootRF_standard_uncertainty': (np.exp(y_upper) - np.exp(y_lower)) / np.mean(np.exp(y_upper) + np.exp(y_lower)),\n",
    "    'BootRF_test_oc': np.exp(y_test),\n",
    "    'BootRF_Point_ID': point_id\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oc.to_csv('C:\\\\Users\\\\nkakhani\\\\_CP_DSM\\\\Conformal_Prediction_DSM\\\\.csv', index = False)  # Set index=False to exclude row indices in the CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quntile Regression Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skgarden import RandomForestQuantileRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits = 5, random_state = 11)\n",
    "rfqr = RandomForestQuantileRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1       21.9\n",
       "2       18.4\n",
       "3       48.0\n",
       "4       25.2\n",
       "5       16.4\n",
       "        ... \n",
       "9995    42.7\n",
       "9996    40.3\n",
       "9997    16.5\n",
       "9998    39.3\n",
       "9999    27.3\n",
       "Name: OC, Length: 9999, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-d339ae8da9e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mrfqr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mrfqr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0my_true_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mupper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrfqr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquantile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m98.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\skgarden\\quantile\\ensemble.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     78\u001b[0m         X, y = check_X_y(\n\u001b[0;32m     79\u001b[0m             X, y, accept_sparse=\"csc\", dtype=np.float32, multi_output=False)\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseForestQuantileRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 330\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 264\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 264\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\skgarden\\quantile\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m     95\u001b[0m         super(BaseTreeQuantileRegressor, self).fit(\n\u001b[0;32m     96\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\nkakhani\\miniconda3\\envs\\conformal\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "y_true_all = []\n",
    "lower = []\n",
    "upper = []\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test, y_train, y_test = (\n",
    "        X[train_index], X[test_index], y[train_index], y[test_index])\n",
    "\n",
    "    rfqr.set_params(max_features=X_train.shape[1] // 3)\n",
    "    rfqr.fit(X_train, y_train)\n",
    "    y_true_all = np.concatenate((y_true_all, y_test))\n",
    "    upper = np.concatenate((upper, rfqr.predict(X_test, quantile=98.5)))\n",
    "    lower = np.concatenate((lower, rfqr.predict(X_test, quantile=2.5)))\n",
    "\n",
    "interval = upper - lower\n",
    "sort_ind = np.argsort(interval)\n",
    "y_true_all = y_true_all[sort_ind]\n",
    "upper = upper[sort_ind]\n",
    "lower = lower[sort_ind]\n",
    "mean = (upper + lower) / 2\n",
    "\n",
    "coverage_QRF, length_QRF = helper.compute_coverage(y_true_all,lower,upper,alpha,\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define QRF model\n",
    "quantile_estimator = helper.QuantileForestRegressorAdapter(model=None,\n",
    "                                                           fit_params=None,\n",
    "                                                           quantiles=quantiles_forest,\n",
    "                                                           params=params_qforest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CQR object\n",
    "nc = RegressorNc(quantile_estimator, QuantileRegErrFunc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run CQR procedure\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale your results back to their original scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lower_rescaled = np.exp(y_lower) \n",
    "y_upper_rescaled = np.exp(y_upper) \n",
    "y_test_rescaled = np.exp(y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics for UQ methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PICP\n",
    "def calculate_picp(true_values, lower_bounds, upper_bounds):\n",
    "    num_samples = len(true_values)\n",
    "    within_interval = np.logical_and(true_values >= lower_bounds, true_values <= upper_bounds)\n",
    "    picp = np.sum(within_interval) / num_samples\n",
    "    return picp\n",
    "\n",
    "picp = calculate_picp(y_test_rescaled, y_lower_rescaled, y_upper_rescaled)\n",
    "print(f'PICP: {picp:.2f}')\n",
    "\n",
    "# Calculate PINAW\n",
    "def calculate_pinaw(lower_bounds, upper_bounds):\n",
    "    pinaw = np.mean(upper_bounds - lower_bounds)\n",
    "    return pinaw\n",
    "\n",
    "pinaw = calculate_pinaw(y_lower_rescaled, y_upper_rescaled)\n",
    "print(f'PINAW: {pinaw:.2f}')\n",
    "\n",
    "# Calculate CWC\n",
    "def calculate_cwc(true_values, lower_bounds, upper_bounds):\n",
    "    within_interval = np.logical_and(true_values >= lower_bounds, true_values <= upper_bounds)\n",
    "    cwc = np.mean(upper_bounds[within_interval] - lower_bounds[within_interval])\n",
    "    return cwc\n",
    "\n",
    "cwc = calculate_cwc(y_test_rescaled, y_lower_rescaled, y_upper_rescaled)\n",
    "print(f'CWC: {cwc:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.plot_func_data(y_test,y_lower,y_upper,\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the desired columns\n",
    "df_oc = pd.DataFrame({\n",
    "    'lower_oc': y_lower_rescaled,\n",
    "    'upper_oc': y_upper_rescaled,\n",
    "    'predicted_oc': (y_upper_rescaled + y_lower_rescaled)/2,\n",
    "    'standard_uncertainty': (y_upper_rescaled - y_lower_rescaled) / np.mean((y_upper_rescaled + y_lower_rescaled)),\n",
    "    'test_oc': y_test_rescaled,\n",
    "    'Point_ID': point_id\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_oc.to_csv('D:\\Conformalized_Quantile_Regression\\LUCAS_2015_cqr.csv', index = False)  # Set index=False to exclude row indices in the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print average coverage and average length\n",
    "coverage_cp_qforest, length_cp_qforest = helper.compute_coverage(y_test,\n",
    "                                                                 y_lower,\n",
    "                                                                 y_upper,\n",
    "                                                                 alpha,\n",
    "                                                                 \"CQR Random Forests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, we obtained valid coverage.\n",
    "\n",
    "### Asymmetric nonconformity score \n",
    "\n",
    "The nonconformity score function `QuantileRegErrFunc` treats the left and right tails symmetrically, but if the error distribution is significantly skewed, one may choose to treat them asymmetrically. This can be done by replacing `QuantileRegErrFunc` with `QuantileRegAsymmetricErrFunc`, as implemented in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nonconformist.nc import QuantileRegAsymmetricErrFunc\n",
    "\n",
    "# # define QRF model\n",
    "# quantile_estimator = helper.QuantileForestRegressorAdapter(model=None,\n",
    "#                                                            fit_params=None,\n",
    "#                                                            quantiles=quantiles_forest,\n",
    "#                                                            params=params_qforest)\n",
    "        \n",
    "# # define the CQR object\n",
    "# nc = RegressorNc(quantile_estimator, QuantileRegAsymmetricErrFunc())\n",
    "\n",
    "# # run CQR procedure\n",
    "# y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "# # compute and print average coverage and average length\n",
    "# coverage_cp_qforest, length_cp_qforest = helper.compute_coverage(y_test,\n",
    "#                                                                  y_lower,\n",
    "#                                                                  y_upper,\n",
    "#                                                                  alpha,\n",
    "#                                                                  \"Asymmetric CQR Random Forests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we also obtained valid coverage.\n",
    "\n",
    "\n",
    "## CQR neural net\n",
    "\n",
    "In what follows we will use neural network as the underlying quantile regression method. Below, we set the hyper-parameters of the CQR neural network method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# Neural network parameters\n",
    "# (See AllQNet_RegressorAdapter class in helper.py)\n",
    "#####################################################\n",
    "\n",
    "# pytorch's optimizer object\n",
    "nn_learn_func = torch.optim.Adam\n",
    "\n",
    "# number of epochs\n",
    "epochs = 1000\n",
    "\n",
    "# learning rate\n",
    "lr = 0.0005\n",
    "\n",
    "# mini-batch size\n",
    "batch_size = 64\n",
    "\n",
    "# hidden dimension of the network\n",
    "hidden_size = 64\n",
    "\n",
    "# dropout regularization rate\n",
    "dropout = 0.1\n",
    "\n",
    "# weight decay regularization\n",
    "wd = 1e-6\n",
    "\n",
    "# Ask for a reduced coverage when tuning the network parameters by \n",
    "# cross-validataion to avoid too concervative initial estimation of the \n",
    "# prediction interval. This estimation will be conformalized by CQR.\n",
    "quantiles_net = [0.1, 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now turn to invoke the CQR procedure. The class `AllQNet_RegressorAdapter` defines the underlying neural network estimator. Just as before, `RegressorNc` defines the CQR objecct, which uses `QuantileRegErrFunc` as the nonconformity score. The function `run_icp` returns the conformal band, computed on test data. Lastly, we compute the average coverage and length using `compute_coverage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantile neural network model\n",
    "quantile_estimator = helper.AllQNet_RegressorAdapter(model=None,\n",
    "                                                     fit_params=None,\n",
    "                                                     in_shape=in_shape,\n",
    "                                                     hidden_size=hidden_size,\n",
    "                                                     quantiles=quantiles_net,\n",
    "                                                     learn_func=nn_learn_func,\n",
    "                                                     epochs=epochs,\n",
    "                                                     batch_size=batch_size,\n",
    "                                                     dropout=dropout,\n",
    "                                                     lr=lr,\n",
    "                                                     wd=wd,\n",
    "                                                     test_ratio=cv_test_ratio,\n",
    "                                                     random_state=cv_random_state,\n",
    "                                                     use_rearrangement=False)\n",
    "\n",
    "# define a CQR object, computes the absolute residual error of points \n",
    "# located outside the estimated quantile neural network band \n",
    "nc = RegressorNc(quantile_estimator, QuantileRegErrFunc())\n",
    "\n",
    "# run CQR procedure\n",
    "y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "# compute and print average coverage and average length\n",
    "coverage_cp_qnet, length_cp_qnet = helper.compute_coverage(y_test,\n",
    "                                                           y_lower,\n",
    "                                                           y_upper,\n",
    "                                                           alpha,\n",
    "                                                           \"CQR Neural Net\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that the prediction interval constructed by CQR Neural Net is also valid. Notice the difference in the average length between the two methods (CQR Neural Net and CQR Random Forests). \n",
    "\n",
    "## CQR neural net with rearrangement\n",
    "\n",
    "Crossing quantiles is a longstanding problem in quantile regression. This issue does not affect the validity guarantee of CQR as it holds regardless of the accuracy or choice of the quantile regression method. However, this may affect the effeciency of the resulting conformal band.\n",
    "\n",
    "Below we use the rearrangement method [3] to bypass the crossing quantile problem. Notice that we pass `use_rearrangement=True` as an argument to `AllQNet_RegressorAdapter`.\n",
    "\n",
    "[3] Chernozhukov Victor, Iván Fernández‐Val, and Alfred Galichon. “Quantile and probability curves without crossing.” Econometrica 78, no. 3 (2010): 1093-1125."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define quantile neural network model, using the rearrangement algorithm\n",
    "# quantile_estimator = helper.AllQNet_RegressorAdapter(model=None,\n",
    "#                                                      fit_params=None,\n",
    "#                                                      in_shape=in_shape,\n",
    "#                                                      hidden_size=hidden_size,\n",
    "#                                                      quantiles=quantiles_net,\n",
    "#                                                      learn_func=nn_learn_func,\n",
    "#                                                      epochs=epochs,\n",
    "#                                                      batch_size=batch_size,\n",
    "#                                                      dropout=dropout,\n",
    "#                                                      lr=lr,\n",
    "#                                                      wd=wd,\n",
    "#                                                      test_ratio=cv_test_ratio,\n",
    "#                                                      random_state=cv_random_state,\n",
    "#                                                      use_rearrangement=True)\n",
    "\n",
    "# # define the CQR object, computing the absolute residual error of points \n",
    "# # located outside the estimated quantile neural network band \n",
    "# nc = RegressorNc(quantile_estimator, QuantileRegErrFunc())\n",
    "\n",
    "# # run CQR procedure\n",
    "# y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "# # compute and print average coverage and average length\n",
    "# coverage_cp_re_qnet, length_cp_re_qnet = helper.compute_coverage(y_test,\n",
    "#                                                                  y_lower,\n",
    "#                                                                  y_upper,\n",
    "#                                                                  alpha,\n",
    "#                                                                  \"CQR Rearrangement Neural Net\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
