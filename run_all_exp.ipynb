{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains all the required experiments \n",
    "\n",
    "Project: Conformal prediction for digital soil mapping\n",
    "\n",
    "Author: Nafiseh Kakhani, University of Tuebingen\n",
    "\n",
    "Date: 16/10/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.metrics import mean_pinball_loss, mean_squared_error\n",
    "import random\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('D:\\Conformalized_Quantile_Regression\\LUCAS_2015_features_V3.csv')\n",
    "df = pd.read_csv('D:\\Conformal_prediction_all_exprs\\Conformal_Prediction_DSM\\LUCAS_2015_features_V3.csv')\n",
    "\n",
    "# Convert the column to numeric, omitting non-double values\n",
    "df['OC'] = pd.to_numeric(df['OC'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (non-double values)\n",
    "df.dropna(subset=['OC'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inputs and output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[0:100,:-2]\n",
    "y = df.iloc[0:100,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [123, 450, 45, 609]\n",
    "\n",
    "# random_state_train_test = seed\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# desired miscoverage error\n",
    "alpha = 0.1\n",
    "\n",
    "# desired quanitile levels\n",
    "quantiles = [0.05, 0.95]\n",
    "\n",
    "# used to determine the size of test set\n",
    "test_ratio = 0.2\n",
    "train_ratio = 0.8 #this value indicates the train ratio with regard to the val/cal ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_test(X, y, test_ratio):\n",
    "\n",
    "    # Divide the dataset into test and train based on the test_ratio parameter\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
    "    \n",
    "    # Let us keep the indices for test samples for further mapping\n",
    "    keep_inds = x_test.index.tolist()\n",
    "    point_id = df.loc[keep_inds, 'point_id']\n",
    "    \n",
    "    # Reshape the data\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    # Compute input dimensions\n",
    "    n_train = x_train.shape[0]\n",
    "    in_shape = x_train.shape[1]\n",
    "    \n",
    "    # # Display basic information\n",
    "    # print(\"Dimensions: train set (n=%d, p=%d) ; test set (n=%d, p=%d)\" % \n",
    "    #       (x_train.shape[0], x_train.shape[1], x_test.shape[0], x_test.shape[1]))\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, point_id, n_train\n",
    "\n",
    "# # Example usage:\n",
    "# x_train, x_test, y_train, y_test, point_id, in_shape = split_train_test(X, y, test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, point_id, n_train = split_train_test(X, y, test_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train):\n",
    "\n",
    "    # Divide the data into proper training set and calibration set\n",
    "    idx = np.random.permutation(n_train)\n",
    "    split_point = int(np.floor(n_train * train_ratio))\n",
    "    \n",
    "    # Split the indices into training and calibration sets\n",
    "    idx_train, idx_cal = idx[:split_point], idx[split_point:]\n",
    "\n",
    "    # Zero mean and unit variance scaling\n",
    "    scalerX = StandardScaler()\n",
    "    scalerX = scalerX.fit(x_train[idx_train])\n",
    "\n",
    "    # Scale\n",
    "    x_train = scalerX.transform(x_train)\n",
    "    x_test = scalerX.transform(x_test)\n",
    "\n",
    "    # # Optionally scale the labels by dividing each by the mean absolute response\n",
    "    # if mean_response:\n",
    "    #     mean_y_train = np.mean(np.abs(y_train[idx_train]))\n",
    "    #     y_train = np.squeeze(y_train) / mean_y_train\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Dimensions: train set (n=%d, p=%d) ; validation/calibration set (n=%d, p=%d)\" % \n",
    "          (x_train.shape[0], x_train.shape[1], x_test.shape[0], x_test.shape[1]))\n",
    "\n",
    "    # Using log transformation to see whether the results are improved\n",
    "    y_train = np.log(np.squeeze(y_train))\n",
    "    y_test = np.log(np.squeeze(y_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, idx_train, idx_cal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions: train set (n=80, p=72) ; validation/calibration set (n=20, p=72)\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "x_train, x_test, y_train, y_test, idx_train, idx_cal = preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.nc import QuantileRegErrFunc\n",
    "\n",
    "#########################################################\n",
    "# Quantile random forests parameters\n",
    "# (See QuantileForestRegressorAdapter class in helper.py)\n",
    "#########################################################\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 1000\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = x_train.shape[1]\n",
    "\n",
    "# target quantile levels\n",
    "quantiles_forest = [quantiles[0]*100, quantiles[1]*100]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 0.85\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = 1\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 10\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file):\n",
    "\n",
    "    df_oc = pd.DataFrame({\n",
    "        'lower_oc': np.exp(y_lower),\n",
    "        'upper_oc': np.exp(y_upper),\n",
    "        'predicted_oc': (np.exp(y_upper) + np.exp(y_lower)) / 2,\n",
    "        'standard_uncertainty': (np.exp(y_upper) - np.exp(y_lower)) / np.mean(np.exp(y_upper) + np.exp(y_lower)),\n",
    "        'test_oc': np.exp(y_test),\n",
    "        'Point_ID': point_id\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df_oc.to_csv(output_file, index=False)\n",
    "\n",
    "# Example usage:\n",
    "# create_and_save_dataframe(y_lower, y_upper, y_test, point_id, 'output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_models(x_train, y_train, x_test, point_id):\n",
    "\n",
    "    # Reference: https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html\n",
    "\n",
    "    \n",
    "    all_models = {}\n",
    "    common_params = dict(\n",
    "        learning_rate = 0.05,\n",
    "        n_estimators = 200,\n",
    "        max_depth = 2,\n",
    "        min_samples_leaf = 9,\n",
    "        min_samples_split = 9,\n",
    "    )\n",
    "\n",
    "    for alpha_gb in [0.05, 0.5, 0.95]:\n",
    "        gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha_gb, **common_params)\n",
    "        all_models[\"q %1.2f\" % alpha_gb] = gbr.fit(x_train, y_train)\n",
    "    \n",
    "    # For the sake of comparison, we also fit a baseline model trained with the usual (mean) squared error (MSE).\n",
    "    # gbr_ls = GradientBoostingRegressor(loss=\"squared_error\", **common_params)\n",
    "    # all_models[\"mse\"] = gbr_ls.fit(x_train, y_train)\n",
    "\n",
    "    y_lower = all_models[\"q 0.05\"].predict(x_test)\n",
    "    y_upper = all_models[\"q 0.95\"].predict(x_test)\n",
    "\n",
    "    return y_lower, y_upper, point_id\n",
    "\n",
    "# Example usage:\n",
    "# trained_models, lower_predictions, upper_predictions = train_gradient_boosting_models(x_train, y_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_qforest = dict()\n",
    "params_qforest[\"random_state\"] = 0\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"max_features\"] = x_train.shape[1]\n",
    "\n",
    "params_qforest[\"CV\"]= cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"]=cv_test_ratio\n",
    "params_qforest[\"random_state\"]=cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Dimensions: train set (n=80, p=72) ; validation/calibration set (n=20, p=72)\n",
      "Ridge relative error: 0.081019\n",
      "Iteration 1\n",
      "Dimensions: train set (n=80, p=72) ; validation/calibration set (n=20, p=72)\n",
      "Ridge relative error: 0.055656\n",
      "Iteration 2\n",
      "Dimensions: train set (n=80, p=72) ; validation/calibration set (n=20, p=72)\n",
      "Ridge relative error: 0.073776\n",
      "Iteration 3\n",
      "Dimensions: train set (n=80, p=72) ; validation/calibration set (n=20, p=72)\n",
      "Ridge relative error: 0.041950\n"
     ]
    }
   ],
   "source": [
    "for i, seed in enumerate(seeds):\n",
    "  \n",
    "  # Set random seeds for reproducibility\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  print(f\"Iteration {i}\")\n",
    "  \n",
    "  x_train, x_test, y_train, y_test, point_id, n_train = split_train_test(X, y, test_ratio)\n",
    "  # Now we split the data in to different train/cal stes:\n",
    "  x_train, x_test, y_train, y_test, idx_train, idx_cal = preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train)\n",
    "\n",
    "  #########################################################\n",
    "  # fit a simple ridge regression model (sanity check)\n",
    "  #########################################################\n",
    "  model = linear_model.RidgeCV()\n",
    "  model = model.fit(x_train, np.squeeze(y_train))\n",
    "  predicted_data = model.predict(x_test).astype(np.float32)\n",
    "\n",
    "  # calculate the normalized mean squared error\n",
    "  print(\"Ridge relative error: %f\" % (np.sum((np.squeeze(y_test)-predicted_data)**2)/np.sum(np.squeeze(y_test)**2)))\n",
    "  sys.stdout.flush()\n",
    "\n",
    "  #########################################################\n",
    "  # Random forests + Bootstrapping\n",
    "  #########################################################\n",
    "\n",
    "  # Initialize lists to store predictions for each bootstrap sample\n",
    "  bootstrap_predictions = []\n",
    "\n",
    "  # Standard bootstrap: Omit around 36.8% of samples in each iteration\n",
    "  sample_size = int(0.632 * x_train.shape[0])  # 63.2% of training samples\n",
    "    \n",
    "  for _ in range(n_bootstraps):\n",
    "        # Create a new bootstrap sample by randomly selecting samples with replacement\n",
    "        bootstrap_indices = np.random.choice(x_train.shape[0], size=sample_size, replace=True)\n",
    "        X_bootstrap = x_train[bootstrap_indices]\n",
    "        y_bootstrap = y_train[bootstrap_indices]\n",
    "        \n",
    "        # Create and train a new model on the bootstrap sample\n",
    "        bootstrap_model = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, random_state = 42)\n",
    "        bootstrap_model.fit(X_bootstrap, y_bootstrap)\n",
    "        \n",
    "        # Make predictions on the test data using the bootstrap model\n",
    "        y_bootstrap_pred = bootstrap_model.predict(x_test)\n",
    "        \n",
    "        # Append the predictions to the list\n",
    "        bootstrap_predictions.append(y_bootstrap_pred)\n",
    "    \n",
    "\n",
    "  y_upper = np.max(bootstrap_predictions, axis=0)\n",
    "  y_lower = np.min(bootstrap_predictions, axis=0)\n",
    "\n",
    "  output_file = f'Boot_RF_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "\n",
    " #########################################################\n",
    " # Quantile random forests \n",
    " #########################################################\n",
    "\n",
    "  model_full = helper.QuantileForestRegressorAdapter(model = None,\n",
    "                                                    fit_params=None,\n",
    "                                                    quantiles=np.dot(100,quantiles),\n",
    "                                                    params = params_qforest)\n",
    "  \n",
    "  model_full.fit(x_train, y_train)\n",
    "  tmp = model_full.predict(x_test)\n",
    "\n",
    "  y_lower = tmp[:,0]\n",
    "  y_upper = tmp[:,1]\n",
    "\n",
    "  output_file = f'quantile_forest_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "\n",
    "  #########################################################\n",
    "  # Quantile random forests + Conformal predicition \n",
    "  #########################################################\n",
    "\n",
    "  model = helper.QuantileForestRegressorAdapter(model = None,\n",
    "                                              fit_params=None,\n",
    "                                              quantiles=quantiles_forest,\n",
    "                                              params = params_qforest)\n",
    "\n",
    "  nc = RegressorNc(model, QuantileRegErrFunc())\n",
    "  y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "\n",
    "  output_file = f'cqr_quantile_forest_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "  \n",
    "  #########################################################\n",
    "  # Prediction Intervals for Gradient Boosting Regression \n",
    "  #########################################################\n",
    "\n",
    "  y_lower, y_upper, point_id = train_gradient_boosting_models(x_train, y_train, x_test, point_id)\n",
    "  output_file = f'quantile_GBoost_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
