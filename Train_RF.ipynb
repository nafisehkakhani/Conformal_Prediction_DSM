{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('D:\\Conformalized_Quantile_Regression\\LUCAS_2015_features_V3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the column to numeric, omitting non-double values\n",
    "df['OC'] = pd.to_numeric(df['OC'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (non-double values)\n",
    "df.dropna(subset=['OC'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SR_B3_1</th>\n",
       "      <th>SR_B3_2</th>\n",
       "      <th>SR_B3_3</th>\n",
       "      <th>SR_B3_4</th>\n",
       "      <th>SR_B4_1</th>\n",
       "      <th>SR_B4_2</th>\n",
       "      <th>SR_B4_3</th>\n",
       "      <th>SR_B4_4</th>\n",
       "      <th>SR_B5_1</th>\n",
       "      <th>SR_B5_2</th>\n",
       "      <th>...</th>\n",
       "      <th>average_7</th>\n",
       "      <th>average_8</th>\n",
       "      <th>average_9</th>\n",
       "      <th>average_10</th>\n",
       "      <th>average_11</th>\n",
       "      <th>average_12</th>\n",
       "      <th>average_13</th>\n",
       "      <th>average_14</th>\n",
       "      <th>OC</th>\n",
       "      <th>point_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.065401</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.063259</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.058280</td>\n",
       "      <td>0.053137</td>\n",
       "      <td>0.053137</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>0.341738</td>\n",
       "      <td>...</td>\n",
       "      <td>532.638051</td>\n",
       "      <td>498.618622</td>\n",
       "      <td>502.788853</td>\n",
       "      <td>704.755629</td>\n",
       "      <td>402.898849</td>\n",
       "      <td>469.73883</td>\n",
       "      <td>617.714286</td>\n",
       "      <td>396.468312</td>\n",
       "      <td>24.6</td>\n",
       "      <td>26581768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037099</td>\n",
       "      <td>0.036429</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.035307</td>\n",
       "      <td>0.034250</td>\n",
       "      <td>0.033989</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.032337</td>\n",
       "      <td>0.179388</td>\n",
       "      <td>0.175070</td>\n",
       "      <td>...</td>\n",
       "      <td>512.001688</td>\n",
       "      <td>489.344548</td>\n",
       "      <td>512.317155</td>\n",
       "      <td>678.640244</td>\n",
       "      <td>404.306692</td>\n",
       "      <td>481.17483</td>\n",
       "      <td>593.808163</td>\n",
       "      <td>402.009979</td>\n",
       "      <td>21.9</td>\n",
       "      <td>26581792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.072997</td>\n",
       "      <td>0.075304</td>\n",
       "      <td>0.071491</td>\n",
       "      <td>0.072426</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>0.065369</td>\n",
       "      <td>0.060863</td>\n",
       "      <td>0.061394</td>\n",
       "      <td>0.347341</td>\n",
       "      <td>0.352894</td>\n",
       "      <td>...</td>\n",
       "      <td>519.103506</td>\n",
       "      <td>509.807511</td>\n",
       "      <td>457.939797</td>\n",
       "      <td>668.159475</td>\n",
       "      <td>448.914535</td>\n",
       "      <td>505.68683</td>\n",
       "      <td>625.240816</td>\n",
       "      <td>395.747479</td>\n",
       "      <td>18.4</td>\n",
       "      <td>26581954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.026582</td>\n",
       "      <td>0.026362</td>\n",
       "      <td>0.026356</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.022784</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.022424</td>\n",
       "      <td>0.022880</td>\n",
       "      <td>0.189531</td>\n",
       "      <td>0.186850</td>\n",
       "      <td>...</td>\n",
       "      <td>520.863506</td>\n",
       "      <td>495.344548</td>\n",
       "      <td>490.902061</td>\n",
       "      <td>686.594090</td>\n",
       "      <td>401.957672</td>\n",
       "      <td>480.77883</td>\n",
       "      <td>613.922449</td>\n",
       "      <td>398.876646</td>\n",
       "      <td>48.0</td>\n",
       "      <td>26601784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051178</td>\n",
       "      <td>0.051178</td>\n",
       "      <td>0.047617</td>\n",
       "      <td>0.047617</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.031081</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.358772</td>\n",
       "      <td>0.358772</td>\n",
       "      <td>...</td>\n",
       "      <td>497.983506</td>\n",
       "      <td>490.677881</td>\n",
       "      <td>449.362438</td>\n",
       "      <td>634.390244</td>\n",
       "      <td>435.981202</td>\n",
       "      <td>489.49483</td>\n",
       "      <td>594.461224</td>\n",
       "      <td>388.101646</td>\n",
       "      <td>25.2</td>\n",
       "      <td>26601978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21853</th>\n",
       "      <td>0.073742</td>\n",
       "      <td>0.073323</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.073102</td>\n",
       "      <td>0.110663</td>\n",
       "      <td>0.109788</td>\n",
       "      <td>0.111120</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.185107</td>\n",
       "      <td>0.184097</td>\n",
       "      <td>...</td>\n",
       "      <td>644.881688</td>\n",
       "      <td>600.148252</td>\n",
       "      <td>477.336023</td>\n",
       "      <td>746.617167</td>\n",
       "      <td>403.169437</td>\n",
       "      <td>416.83883</td>\n",
       "      <td>690.440816</td>\n",
       "      <td>344.026646</td>\n",
       "      <td>8.4</td>\n",
       "      <td>64881666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21854</th>\n",
       "      <td>0.085582</td>\n",
       "      <td>0.085886</td>\n",
       "      <td>0.086188</td>\n",
       "      <td>0.086588</td>\n",
       "      <td>0.071660</td>\n",
       "      <td>0.072601</td>\n",
       "      <td>0.071150</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>0.402217</td>\n",
       "      <td>0.400485</td>\n",
       "      <td>...</td>\n",
       "      <td>636.685324</td>\n",
       "      <td>590.337140</td>\n",
       "      <td>476.524702</td>\n",
       "      <td>736.836398</td>\n",
       "      <td>395.087084</td>\n",
       "      <td>410.73883</td>\n",
       "      <td>680.738776</td>\n",
       "      <td>341.705812</td>\n",
       "      <td>10.8</td>\n",
       "      <td>64901668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21855</th>\n",
       "      <td>0.084536</td>\n",
       "      <td>0.087933</td>\n",
       "      <td>0.081859</td>\n",
       "      <td>0.085314</td>\n",
       "      <td>0.071451</td>\n",
       "      <td>0.077255</td>\n",
       "      <td>0.066329</td>\n",
       "      <td>0.072025</td>\n",
       "      <td>0.391538</td>\n",
       "      <td>0.383660</td>\n",
       "      <td>...</td>\n",
       "      <td>636.685324</td>\n",
       "      <td>590.337140</td>\n",
       "      <td>476.524702</td>\n",
       "      <td>736.836398</td>\n",
       "      <td>395.087084</td>\n",
       "      <td>410.73883</td>\n",
       "      <td>680.738776</td>\n",
       "      <td>341.705812</td>\n",
       "      <td>6.7</td>\n",
       "      <td>64901672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21856</th>\n",
       "      <td>0.087019</td>\n",
       "      <td>0.087398</td>\n",
       "      <td>0.086868</td>\n",
       "      <td>0.087840</td>\n",
       "      <td>0.099747</td>\n",
       "      <td>0.102079</td>\n",
       "      <td>0.099499</td>\n",
       "      <td>0.102728</td>\n",
       "      <td>0.286553</td>\n",
       "      <td>0.283027</td>\n",
       "      <td>...</td>\n",
       "      <td>648.761688</td>\n",
       "      <td>602.381585</td>\n",
       "      <td>490.090740</td>\n",
       "      <td>754.286398</td>\n",
       "      <td>404.941986</td>\n",
       "      <td>417.09883</td>\n",
       "      <td>695.187755</td>\n",
       "      <td>351.993312</td>\n",
       "      <td>5.7</td>\n",
       "      <td>64961676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21857</th>\n",
       "      <td>0.077897</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.078162</td>\n",
       "      <td>0.077860</td>\n",
       "      <td>0.074805</td>\n",
       "      <td>0.074107</td>\n",
       "      <td>0.075763</td>\n",
       "      <td>0.075339</td>\n",
       "      <td>0.265865</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>...</td>\n",
       "      <td>645.212597</td>\n",
       "      <td>597.070474</td>\n",
       "      <td>487.453004</td>\n",
       "      <td>744.874859</td>\n",
       "      <td>399.334143</td>\n",
       "      <td>417.05883</td>\n",
       "      <td>688.228571</td>\n",
       "      <td>348.451646</td>\n",
       "      <td>18.1</td>\n",
       "      <td>64981672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21858 rows Ã— 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SR_B3_1   SR_B3_2   SR_B3_3   SR_B3_4   SR_B4_1   SR_B4_2   SR_B4_3  \\\n",
       "0      0.065401  0.065401  0.063259  0.063259  0.058280  0.058280  0.053137   \n",
       "1      0.037099  0.036429  0.035900  0.035307  0.034250  0.033989  0.032527   \n",
       "2      0.072997  0.075304  0.071491  0.072426  0.063204  0.065369  0.060863   \n",
       "3      0.026582  0.026362  0.026356  0.026522  0.022784  0.022857  0.022424   \n",
       "4      0.051178  0.051178  0.047617  0.047617  0.031081  0.031081  0.028169   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21853  0.073742  0.073323  0.073456  0.073102  0.110663  0.109788  0.111120   \n",
       "21854  0.085582  0.085886  0.086188  0.086588  0.071660  0.072601  0.071150   \n",
       "21855  0.084536  0.087933  0.081859  0.085314  0.071451  0.077255  0.066329   \n",
       "21856  0.087019  0.087398  0.086868  0.087840  0.099747  0.102079  0.099499   \n",
       "21857  0.077897  0.077503  0.078162  0.077860  0.074805  0.074107  0.075763   \n",
       "\n",
       "        SR_B4_4   SR_B5_1   SR_B5_2  ...   average_7   average_8   average_9  \\\n",
       "0      0.053137  0.341738  0.341738  ...  532.638051  498.618622  502.788853   \n",
       "1      0.032337  0.179388  0.175070  ...  512.001688  489.344548  512.317155   \n",
       "2      0.061394  0.347341  0.352894  ...  519.103506  509.807511  457.939797   \n",
       "3      0.022880  0.189531  0.186850  ...  520.863506  495.344548  490.902061   \n",
       "4      0.028169  0.358772  0.358772  ...  497.983506  490.677881  449.362438   \n",
       "...         ...       ...       ...  ...         ...         ...         ...   \n",
       "21853  0.110324  0.185107  0.184097  ...  644.881688  600.148252  477.336023   \n",
       "21854  0.072110  0.402217  0.400485  ...  636.685324  590.337140  476.524702   \n",
       "21855  0.072025  0.391538  0.383660  ...  636.685324  590.337140  476.524702   \n",
       "21856  0.102728  0.286553  0.283027  ...  648.761688  602.381585  490.090740   \n",
       "21857  0.075339  0.265865  0.267717  ...  645.212597  597.070474  487.453004   \n",
       "\n",
       "       average_10  average_11  average_12  average_13  average_14    OC  \\\n",
       "0      704.755629  402.898849   469.73883  617.714286  396.468312  24.6   \n",
       "1      678.640244  404.306692   481.17483  593.808163  402.009979  21.9   \n",
       "2      668.159475  448.914535   505.68683  625.240816  395.747479  18.4   \n",
       "3      686.594090  401.957672   480.77883  613.922449  398.876646  48.0   \n",
       "4      634.390244  435.981202   489.49483  594.461224  388.101646  25.2   \n",
       "...           ...         ...         ...         ...         ...   ...   \n",
       "21853  746.617167  403.169437   416.83883  690.440816  344.026646   8.4   \n",
       "21854  736.836398  395.087084   410.73883  680.738776  341.705812  10.8   \n",
       "21855  736.836398  395.087084   410.73883  680.738776  341.705812   6.7   \n",
       "21856  754.286398  404.941986   417.09883  695.187755  351.993312   5.7   \n",
       "21857  744.874859  399.334143   417.05883  688.228571  348.451646  18.1   \n",
       "\n",
       "       point_id  \n",
       "0      26581768  \n",
       "1      26581792  \n",
       "2      26581954  \n",
       "3      26601784  \n",
       "4      26601978  \n",
       "...         ...  \n",
       "21853  64881666  \n",
       "21854  64901668  \n",
       "21855  64901672  \n",
       "21856  64961676  \n",
       "21857  64981672  \n",
       "\n",
       "[21858 rows x 74 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-2]\n",
    "y = np.log (df.iloc[:,-2] + 1) #log transformation for outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def compute_coverage(y_test,y_lower,y_upper,significance,name=\"\"):\n",
    "    \"\"\" Compute average coverage and length, and print results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    y_test : numpy array, true labels (n)\n",
    "    y_lower : numpy array, estimated lower bound for the labels (n)\n",
    "    y_upper : numpy array, estimated upper bound for the labels (n)\n",
    "    significance : float, desired significance level\n",
    "    name : string, optional output string (e.g. the method name)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    coverage : float, average coverage\n",
    "    avg_length : float, average length\n",
    "\n",
    "    \"\"\"\n",
    "    in_the_range = np.sum((y_test >= y_lower) & (y_test <= y_upper))\n",
    "    coverage = in_the_range / len(y_test) * 100\n",
    "    print(\"%s: Percentage in the range (expecting %.2f): %f\" % (name, 100 - significance*100, coverage))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    avg_length = abs(np.mean(y_lower - y_upper))\n",
    "    print(\"%s: Average length: %f\" % (name, avg_length))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    return coverage, avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_quantile_coverage_probability(data, true_quantile, confidence_interval):\n",
    "    \"\"\"\n",
    "    Calculate the quantile coverage probability.\n",
    "\n",
    "    Parameters:\n",
    "    - data: NumPy array or list of data points.\n",
    "    - true_quantile: The true quantile value to be estimated (e.g., 0.95 for a 95% quantile).\n",
    "    - confidence_interval: List containing two values [alpha, 1 - alpha], where alpha is the significance level.\n",
    "\n",
    "    Returns:\n",
    "    - coverage_probability: True if the true quantile is within the confidence interval, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the sample quantile\n",
    "    sample_quantile = np.percentile(data, 100 * true_quantile)\n",
    "\n",
    "    # Check if the true quantile falls within the confidence interval\n",
    "    coverage_probability = (sample_quantile >= np.percentile(data, 100 * confidence_interval[0])) and \\\n",
    "                          (sample_quantile <= np.percentile(data, 100 * confidence_interval[1]))\n",
    "\n",
    "    return coverage_probability\n",
    "\n",
    "# # Example usage:\n",
    "# data = np.random.normal(loc=0, scale=1, size=1000)\n",
    "# true_quantile = 0.95\n",
    "# confidence_interval = [0.05, 0.95]\n",
    "# coverage_probability = calculate_quantile_coverage_probability(data, true_quantile, confidence_interval)\n",
    "# print(f\"Coverage Probability: {int(coverage_probability)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Mean Squared Error on Test Set: 0.4663052147773841\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 5\n",
    "\n",
    "n_resample = 500\n",
    "\n",
    "# Initialize lists to store predictions for each bootstrap sample\n",
    "bootstrap_predictions = []\n",
    "\n",
    "# Define a grid of hyperparameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 1000],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a Random Forest regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=8)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Fit a Random Forest model with the best hyperparameters\n",
    "best_rf = RandomForestRegressor(**best_params, random_state=42)\n",
    "\n",
    "\n",
    "# Perform bootstrapping and calculate predictions as before\n",
    "for _ in range(n_bootstraps):\n",
    "    X_boot, y_boot = resample(X_train, y_train, n_samples = len(y_train) - n_resample , random_state=np.random.randint(0, 100))\n",
    "    best_rf.fit(X_boot, y_boot)\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    bootstrap_predictions.append(y_pred)\n",
    "\n",
    "# Calculate the mean and standard deviation of predictions\n",
    "mean_predictions = np.mean(bootstrap_predictions, axis=0)\n",
    "std_predictions = np.std(bootstrap_predictions, axis=0)\n",
    "\n",
    "# Calculate upper and lower prediction bands (e.g., 95% confidence interval)\n",
    "alpha = 0.1  # 95% confidence interval\n",
    "z_score = 1.96  # Z-score for a 95% confidence interval\n",
    "\n",
    "upper_band = np.max(bootstrap_predictions, axis=0)\n",
    "lower_band = np.min(bootstrap_predictions, axis=0)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) on the test set\n",
    "mse = mean_squared_error(y_test, mean_predictions)\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Mean Squared Error on Test Set: {(mse)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boot-RF: Percentage in the range (expecting 90.00): 15.576395\n",
      "Boot-RF: Average length: 0.256380\n"
     ]
    }
   ],
   "source": [
    "coverage_RF, length_RF = compute_coverage(y_test, lower_band, upper_band, alpha, name = \"Boot-RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17476"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_boot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for K-fold\n",
    "\n",
    "x_train = np.asarray(X)\n",
    "y_train = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "# Create your dataset (x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"min_samples_leaf\": [1, 2, 5],\n",
    "    \"n_estimators\": [100, 500, 1000],\n",
    "    \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Perform nested cross-validation (e.g., 5-fold outer, 3-fold inner)\n",
    "outer_cv = KFold(n_splits = 5, shuffle=True, random_state=422)\n",
    "inner_cv = KFold(n_splits = 5, shuffle=True, random_state=422)\n",
    "\n",
    "# Initialize lists to store results\n",
    "mean_mse_scores = []\n",
    "std_mse_scores = []\n",
    "\n",
    "for train_index, test_index in outer_cv.split(x_train):\n",
    "    x_train_outer, x_test_outer = x_train[train_index], x_train[test_index]\n",
    "    y_train_outer, y_test_outer = y_train[train_index], y_train[test_index]\n",
    "\n",
    "    # Initialize the inner loop results\n",
    "    inner_residual_matrix = []\n",
    "    Observed = []\n",
    "    Estimated = []\n",
    "\n",
    "    for inner_train_index, inner_test_index in inner_cv.split(x_train_outer):\n",
    "        x_train_inner, x_val_inner = x_train_outer[inner_train_index], x_train_outer[inner_test_index]\n",
    "        y_train_inner, y_val_inner = y_train_outer[inner_train_index], y_train_outer[inner_test_index]\n",
    "\n",
    "        # Initialize the inner GridSearchCV for hyperparameter tuning\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=RandomForestRegressor(random_state=422),\n",
    "            param_grid=param_grid,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            cv=inner_cv,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        # Fit the GridSearchCV to the inner training data\n",
    "        grid_search.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "        # Get the best hyperparameters from the inner loop\n",
    "        best_params = grid_search.best_params_\n",
    "\n",
    "        # Create a RandomForestRegressor with the best hyperparameters\n",
    "        rf_model = RandomForestRegressor(\n",
    "            random_state=422,\n",
    "            min_samples_leaf=best_params['min_samples_leaf'],\n",
    "            n_estimators=best_params['n_estimators'],\n",
    "            max_features=best_params['max_features']\n",
    "        )\n",
    "\n",
    "        # Fit the model to the inner training data\n",
    "        rf_model.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "        # Make predictions on the inner validation data\n",
    "        predictions = rf_model.predict(x_val_inner)\n",
    "\n",
    "        # Calculate residuals for the inner fold and store them in the inner_residual_matrix\n",
    "        Observed.append(y_val_inner)\n",
    "        Estimated.append(predictions)\n",
    "       \n",
    "        # residuals = y_val_inner - predictions\n",
    "        # inner_residual_matrix.append(residuals)\n",
    "        \n",
    "\n",
    "    # # Calculate the mean and standard deviation of MSE scores for the inner loop results\n",
    "    # inner_mse_scores = [mean_squared_error(y_val_inner, predictions) for predictions in rf_model.staged_predict(x_test_outer)]\n",
    "    # mean_inner_mse = np.mean(inner_mse_scores)\n",
    "    # std_inner_mse = np.std(inner_mse_scores)\n",
    "\n",
    "    # # Append the mean and standard deviation of inner MSE to the lists of outer loop results\n",
    "    # mean_mse_scores.append(mean_inner_mse)\n",
    "    # std_mse_scores.append(std_inner_mse)\n",
    "\n",
    "    # Vertically stack the padded arrays\n",
    "    max_cols = max(len(row) for row in Observed)\n",
    "    stacked_arrays_Observed = np.vstack([np.pad(row, (0, max_cols - len(row)), mode='constant') for row in Observed])\n",
    "    stacked_arrays_Observed = np.array(stacked_arrays_Observed).flatten().reshape(-1, 1)\n",
    "\n",
    "    max_cols = max(len(row) for row in Estimated)\n",
    "    stacked_arrays_Estimated = np.vstack([np.pad(row, (0, max_cols - len(row)), mode='constant') for row in Estimated])\n",
    "    stacked_arrays_Estimated = np.array(stacked_arrays_Estimated).flatten().reshape(-1, 1)\n",
    "\n",
    "    # # Vertically stack the padded arrays\n",
    "    # # stacked_arrays = np.vstack(inner_residual_matrix) \n",
    "\n",
    "    \n",
    "    # Now, you can fit a quantile regression model using the inner_residual_matrix and x_test_outer\n",
    "    quantile_model = sm.QuantReg(stacked_arrays_Observed, stacked_arrays_Estimated)\n",
    "    quantile_results = quantile_model.fit(q=0.5)  # Fit the model for the median (you can choose other quantiles)\n",
    "\n",
    "    # Make predictions on the test data using the quantile regression model\n",
    "    quantile_predictions = quantile_results.predict(y_test_outer)\n",
    "\n",
    "    # Define the quantiles you want to estimate (e.g., 10th and 90th percentiles)\n",
    "    quantiles = [0.9]\n",
    "\n",
    "    # Initialize lists to store lower and upper quantile predictions\n",
    "    lower_quantile_predictions = []\n",
    "    upper_quantile_predictions = []\n",
    "\n",
    "    # Calculate prediction intervals for each quantile\n",
    "    for quantile in quantiles:\n",
    "        print(quantile)\n",
    "        # Use quantile_results.get_prediction to estimate the prediction interval\n",
    "        prediction_interval = quantile_results.get_prediction(y_test_outer).summary_frame(alpha=1-quantile)\n",
    "\n",
    "        # Extract lower and upper bounds of the prediction interval\n",
    "        lower_bound = prediction_interval['obs_ci_lower']\n",
    "        upper_bound = prediction_interval['obs_ci_upper']\n",
    "\n",
    "        coverage_RF, length_RF = compute_coverage(y_test_outer, lower_bound, upper_bound, 1-quantile, name = \"PP-RF\")\n",
    "        \n",
    "\n",
    "        # Append lower and upper bounds to the respective lists\n",
    "        # lower_quantile_predictions.append(lower_bound)\n",
    "        # upper_quantile_predictions.append(upper_bound)\n",
    "\n",
    "    # Now, you have lower and upper quantile predictions for each specified quantile\n",
    "    # You can use these to estimate uncertainty or construct prediction intervals as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "picp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and print average coverage and average length\n",
    "coverage_RF, length_RF = helper.compute_coverage(y_test_rf,\n",
    "                                                 y_lower_rf,\n",
    "                                                 y_upper_rf,\n",
    "                                                 alpha,\n",
    "                                                 \"Random Forests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold, GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# import numpy as np\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# def fit_inner_rf_model(x_train_inner, y_train_inner):\n",
    "#     # Initialize the inner GridSearchCV for hyperparameter tuning\n",
    "#     param_grid = {\n",
    "#         \"min_samples_leaf\": [1, 2, 5],\n",
    "#         \"n_estimators\": [100, 500, 1000],\n",
    "#         \"max_features\": [None, \"sqrt\", \"log2\"]\n",
    "#     }\n",
    "\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=RandomForestRegressor(random_state=422),\n",
    "#         param_grid=param_grid,\n",
    "#         scoring='neg_mean_squared_error',\n",
    "#         cv=inner_cv,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "\n",
    "#     # Fit the GridSearchCV to the inner training data\n",
    "#     grid_search.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "#     # Get the best hyperparameters from the inner loop\n",
    "#     best_params = grid_search.best_params_\n",
    "\n",
    "#     # Create a RandomForestRegressor with the best hyperparameters\n",
    "#     rf_model = RandomForestRegressor(\n",
    "#         random_state=422,\n",
    "#         min_samples_leaf=best_params['min_samples_leaf'],\n",
    "#         n_estimators=best_params['n_estimators'],\n",
    "#         max_features=best_params['max_features']\n",
    "#     )\n",
    "\n",
    "#     # Fit the model to the inner training data\n",
    "#     rf_model.fit(x_train_inner, y_train_inner)\n",
    "\n",
    "#     return rf_model\n",
    "\n",
    "# def calculate_inner_residuals(rf_model, x_val_inner):\n",
    "#     # Make predictions on the inner validation data\n",
    "#     predictions = rf_model.predict(x_val_inner)\n",
    "\n",
    "#     # Calculate residuals for the inner fold\n",
    "#     residuals = y_val_inner - predictions\n",
    "\n",
    "#     return residuals\n",
    "\n",
    "# def fit_outer_quantile_regression(x_train_outer, y_train_outer, inner_residual_matrix):\n",
    "#     quantile = 0.5  # Specify the quantile you want to estimate (e.g., 0.5 for the median)\n",
    "\n",
    "#     # Fit a quantile regression model using x_train_outer and inner_residual_matrix\n",
    "#     quantile_model = sm.QuantReg(y_train_outer, np.column_stack((x_train_outer, inner_residual_matrix)))\n",
    "#     quantile_results = quantile_model.fit(q=quantile)  # Fit the model for the median (you can choose other quantiles)\n",
    "\n",
    "#     return quantile_results\n",
    "\n",
    "# def calculate_prediction_intervals(quantile_results, new_data_x, new_data_outer_residuals):\n",
    "#     quantile = 0.5  # Specify the quantile you want to estimate (e.g., 0.5 for the median)\n",
    "\n",
    "#     # Make predictions on new data using the fitted quantile regression model\n",
    "#     new_data_predictions = quantile_results.predict(np.column_stack((new_data_x, new_data_outer_residuals)))\n",
    "\n",
    "#     # Calculate prediction intervals for the specified quantile using quantile_results.get_prediction\n",
    "#     prediction_interval = quantile_results.get_prediction(np.column_stack((new_data_x, new_data_outer_residuals))).summary_frame(alpha=1-quantile)\n",
    "\n",
    "#     # Extract lower and upper bounds of the prediction interval\n",
    "#     lower_bound = prediction_interval['obs_ci_lower']\n",
    "#     upper_bound = prediction_interval['obs_ci_upper']\n",
    "\n",
    "#     return new_data_predictions, lower_bound, upper_bound\n",
    "\n",
    "# def main():\n",
    "#     # Create your dataset (x_train, y_train, x_test, y_test)\n",
    "\n",
    "#     # Perform nested cross-validation (e.g., 5-fold outer, 3-fold inner)\n",
    "#     outer_cv = KFold(n_splits=5, shuffle=True, random_state=422)\n",
    "#     inner_cv = KFold(n_splits=3, shuffle=True, random_state=422)\n",
    "\n",
    "#     # Initialize lists to store results\n",
    "#     mean_mse_scores = []\n",
    "#     std_mse_scores = []\n",
    "\n",
    "#     # Initialize the outer residual matrix to store residuals from test predictions\n",
    "#     outer_residual_matrix = []\n",
    "\n",
    "#     for train_index, test_index in outer_cv.split(x_train):\n",
    "#         x_train_outer, x_test_outer = x_train[train_index], x_train[test_index]\n",
    "#         y_train_outer, y_test_outer = y_train[train_index], y_train[test_index]\n",
    "\n",
    "#         # Initialize the inner loop results\n",
    "#         inner_residual_matrix = []\n",
    "\n",
    "#         for inner_train_index, inner_test_index in inner_cv.split(x_train_outer):\n",
    "#             x_train_inner, x_val_inner = x_train_outer[inner_train_index], x_train_outer[inner_test_index]\n",
    "#             y_train_inner, y_val_inner = y_train_outer[inner_train_index], y_train_outer[inner_test_index]\n",
    "\n",
    "#             # Fit the inner random forest model\n",
    "#             rf_model = fit_inner_rf_model(x_train_inner, y_train_inner)\n",
    "\n",
    "#             # Calculate residuals for the inner fold and store them in the inner_residual_matrix\n",
    "#             inner_residuals = calculate_inner_residuals(rf_model, x_val_inner)\n",
    "#             inner_residual_matrix.append(inner_residuals)\n",
    "\n",
    "#         # Combine residuals from the inner loop into a single matrix\n",
    "#         inner_residual_matrix = np.vstack(inner_residual_matrix)\n",
    "\n",
    "#         # Fit the outer quantile regression model\n",
    "#         quantile_results = fit_outer_quantile_regression(x_train_outer, y_train_outer, inner_residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "We begin by splitting the data into a proper training set and a calibration set. Recall that the main idea is to fit a regression model on the proper training samples, then use the residuals on a held-out validation set to quantify the uncertainty in future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # divide the data into proper training set and calibration set\n",
    "# idx = np.random.permutation(n_train)\n",
    "# split_point = int(np.floor(n_train * 0.9))\n",
    "# # idx_train, idx_cal = idx[:n_half], idx[n_half:2*n_half]\n",
    "\n",
    "# # Split the indices into training and calibration sets\n",
    "# idx_train, idx_cal = idx[:split_point], idx[split_point:]\n",
    "\n",
    "# # zero mean and unit variance scaling \n",
    "# scalerX = StandardScaler()\n",
    "# scalerX = scalerX.fit(x_train[idx_train])\n",
    "\n",
    "# # scale\n",
    "# x_train = scalerX.transform(x_train)\n",
    "# x_test = scalerX.transform(x_test)\n",
    "\n",
    "# # scale the labels by dividing each by the mean absolute response\n",
    "# mean_y_train = np.mean(np.abs(y_train[idx_train]))\n",
    "# # y_train = np.squeeze(y_train)/mean_y_train\n",
    "\n",
    "# #using log transformation to see whether the results are improved\n",
    "# y_train = np.log(np.squeeze(y_train))\n",
    "# y_test = np.log(np.squeeze(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.exp(y_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
