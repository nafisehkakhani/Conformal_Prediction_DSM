{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains all the required experiments \n",
    "\n",
    "Project: Conformal prediction for digital soil mapping\n",
    "\n",
    "Author: Nafiseh Kakhani, University of Tuebingen\n",
    "\n",
    "Date: 16/10/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from sklearn.linear_model import QuantileRegressor\n",
    "# from sklearn.metrics import mean_pinball_loss, mean_squared_error\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "import random\n",
    "import numpy as np\n",
    "np.warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "import os\n",
    "import sys\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "Please load your input features + output as .csv file in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('path\\\\toyour\\\\csv file')\n",
    "\n",
    "# Convert the column to numeric, omitting non-double values\n",
    "df['OC'] = pd.to_numeric(df['OC'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values (non-double values)\n",
    "df.dropna(subset=['OC'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inputs and output of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-2]\n",
    "y = df.iloc[:,-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cqr import helper\n",
    "from nonconformist.nc import RegressorNc\n",
    "from nonconformist.nc import QuantileRegErrFunc\n",
    "\n",
    "seeds = [123, 450, 45, 609, 700]\n",
    "# seeds = [5660, 968, 111]\n",
    "\n",
    "# random_state_train_test = seed\n",
    "# random.seed(seed)\n",
    "# np.random.seed(seed)\n",
    "# torch.manual_seed(seed)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# desired miscoverage error\n",
    "alpha = 0.1\n",
    "\n",
    "# desired quanitile levels\n",
    "quantiles = [0.05, 0.95]\n",
    "\n",
    "# used to determine the size of test set\n",
    "test_ratio = 0.2\n",
    "train_ratio = 0.95 #this value indicates the train ratio with regard to the val/cal ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_train_test(X, y, test_ratio):\n",
    "\n",
    "    # Divide the dataset into test and train based on the test_ratio parameter\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio)\n",
    "    \n",
    "    # Let us keep the indices for test samples for further mapping\n",
    "    keep_inds = x_test.index.tolist()\n",
    "    point_id = df.loc[keep_inds, 'point_id']\n",
    "    \n",
    "    # Reshape the data\n",
    "    x_train = np.asarray(x_train)\n",
    "    y_train = np.asarray(y_train)\n",
    "    x_test = np.asarray(x_test)\n",
    "    y_test = np.asarray(y_test)\n",
    "    \n",
    "    # Compute input dimensions\n",
    "    n_train = x_train.shape[0]\n",
    "    in_shape = x_train.shape[1]\n",
    "    \n",
    "    # # Display basic information\n",
    "    # print(\"Dimensions: train set (n=%d, p=%d) ; test set (n=%d, p=%d)\" % \n",
    "    #       (x_train.shape[0], x_train.shape[1], x_test.shape[0], x_test.shape[1]))\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, point_id, n_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, point_id, n_train = split_train_test(X, y, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train):\n",
    "\n",
    "    # Divide the data into proper training set and calibration set\n",
    "    idx = np.random.permutation(n_train)\n",
    "    split_point = int(np.floor(n_train * train_ratio))\n",
    "    \n",
    "    # Split the indices into training and calibration sets\n",
    "    idx_train, idx_cal = idx[:split_point], idx[split_point:]\n",
    "\n",
    "    # Zero mean and unit variance scaling\n",
    "    scalerX = StandardScaler()\n",
    "    scalerX = scalerX.fit(x_train[idx_train])\n",
    "\n",
    "    # Scale\n",
    "    x_train = scalerX.transform(x_train)\n",
    "    x_test = scalerX.transform(x_test)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"Dimensions: train set (n=%d, p=%d) ; test set (n=%d, p=%d); cal/val set: (n=%d)\" % \n",
    "          (x_train.shape[0], x_train.shape[1], x_test.shape[0], x_test.shape[1], len(idx_cal)))\n",
    "\n",
    "    # Using log transformation to see whether the results are improved\n",
    "    y_train = np.log(np.squeeze(y_train))\n",
    "    y_test = np.log(np.squeeze(y_test))\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, idx_train, idx_cal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example usage:\n",
    "x_train, x_test, y_train, y_test, idx_train, idx_cal = preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Quantile random forests parameters\n",
    "# (See QuantileForestRegressorAdapter class in helper.py)\n",
    "#########################################################\n",
    "\n",
    "# the number of trees in the forest\n",
    "n_estimators = 1000\n",
    "\n",
    "# the minimum number of samples required to be at a leaf node\n",
    "# (default skgarden's parameter)\n",
    "min_samples_leaf = 1\n",
    "\n",
    "# the number of features to consider when looking for the best split\n",
    "# (default skgarden's parameter)\n",
    "max_features = x_train.shape[1] \n",
    "\n",
    "# target quantile levels\n",
    "quantiles_forest = [quantiles[0]*100, quantiles[1]*100]\n",
    "\n",
    "# use cross-validation to tune the quantile levels?\n",
    "cv_qforest = True\n",
    "\n",
    "# when tuning the two QRF quantile levels one may\n",
    "# ask for a prediction band with smaller average coverage\n",
    "# to avoid too conservative estimation of the prediction band\n",
    "# This would be equal to coverage_factor*(quantiles[1] - quantiles[0])\n",
    "coverage_factor = 0.85\n",
    "\n",
    "# ratio of held-out data, used in cross-validation\n",
    "cv_test_ratio = 0.05\n",
    "\n",
    "# seed for splitting the data in cross-validation.\n",
    "# Also used as the seed in quantile random forests function\n",
    "cv_random_state = 1\n",
    "\n",
    "# determines the lowest and highest quantile level parameters.\n",
    "# This is used when tuning the quanitle levels by cross-validation.\n",
    "# The smallest value is equal to quantiles[0] - range_vals.\n",
    "# Similarly, the largest value is equal to quantiles[1] + range_vals.\n",
    "cv_range_vals = 30\n",
    "\n",
    "# sweep over a grid of length num_vals when tuning QRF's quantile parameters                   \n",
    "cv_num_vals = 5\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_bootstraps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file):\n",
    "\n",
    "    df_oc = pd.DataFrame({\n",
    "        'lower_oc': np.exp(y_lower),\n",
    "        'upper_oc': np.exp(y_upper),\n",
    "        'predicted_oc': (np.exp(y_upper) + np.exp(y_lower)) / 2,\n",
    "        'standard_uncertainty': (np.exp(y_upper) - np.exp(y_lower)) / np.mean(np.exp(y_upper) + np.exp(y_lower)),\n",
    "        'test_oc': np.exp(y_test),\n",
    "        'Point_ID': point_id\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df_oc.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_models(x_train, y_train, x_test, point_id):\n",
    "\n",
    "    # Reference: https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html\n",
    "\n",
    "    # Define a parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'n_estimators': [200, 300],\n",
    "        'max_depth': [1, 2],\n",
    "        'min_samples_split': [2, 3]\n",
    "    }\n",
    "\n",
    "    # Initialize GradientBoostingRegressor with loss=\"quantile\"\n",
    "    gbr = GradientBoostingRegressor(loss=\"quantile\")\n",
    "\n",
    "    # Set up GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(estimator=gbr, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "    grid_search.fit(x_train[idx_cal], y_train[idx_cal])\n",
    "\n",
    "    # Get the best hyperparameters from the grid search\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Train models with the best hyperparameters\n",
    "    all_models = {}\n",
    "    for alpha_gb in [0.05, 0.95]:\n",
    "        gbr = GradientBoostingRegressor(loss=\"quantile\", alpha=alpha_gb, **best_params)\n",
    "        all_models[f\"q {alpha_gb}\"] = gbr.fit(x_train[idx_train], y_train[idx_train])\n",
    "\n",
    "    # Make predictions with the trained models\n",
    "    y_lower = all_models[\"q 0.05\"].predict(x_test)\n",
    "    y_upper = all_models[\"q 0.95\"].predict(x_test)\n",
    "\n",
    "    return y_lower, y_upper, point_id\n",
    "\n",
    "\n",
    "# Other version of the above function without regularization\n",
    "def train_quantile_linear(x_train, y_train, x_test, point_id):\n",
    "\n",
    "   all_models = {}\n",
    "    \n",
    "\n",
    "   for alpha_gb in [0.01, 0.99]:\n",
    "        qr = QuantReg(y_train, x_train)\n",
    "        result = qr.fit(q = alpha_gb)\n",
    "        all_models[\"q %1.2f\" % alpha_gb] = result\n",
    "    \n",
    "\n",
    "   y_lower = all_models[\"q 0.01\"].predict(x_test)\n",
    "   y_upper = all_models[\"q 0.99\"].predict(x_test)\n",
    "\n",
    "   return y_lower, y_upper, point_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom PyTorch module for quantile regression\n",
    "class QuantileRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QuantileRegression, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Define the Pinball Loss function\n",
    "def pinball_loss(y_true, y_pred, quantile):\n",
    "    e = y_true - y_pred\n",
    "    loss = torch.mean(torch.max(quantile * e, (quantile - 1) * e))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_qforest = dict()\n",
    "params_qforest[\"random_state\"] = 3333333\n",
    "params_qforest[\"min_samples_leaf\"] = min_samples_leaf\n",
    "params_qforest[\"n_estimators\"] = n_estimators\n",
    "params_qforest[\"max_features\"] = x_train.shape[1] \n",
    "\n",
    "params_qforest[\"CV\"] = cv_qforest\n",
    "params_qforest[\"coverage_factor\"] = coverage_factor\n",
    "params_qforest[\"test_ratio\"]= cv_test_ratio\n",
    "params_qforest[\"random_state\"]= cv_random_state\n",
    "params_qforest[\"range_vals\"] = cv_range_vals\n",
    "params_qforest[\"num_vals\"] = cv_num_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the directory you want to change to\n",
    "new_directory_path = \"your/direcotry/path\"\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(new_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, seed in enumerate(seeds):\n",
    "  \n",
    "  # Set random seeds for reproducibility\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "\n",
    "  print(f\"Iteration {i}\")\n",
    "  \n",
    "  #Split data into train/test sets\n",
    "  x_train, x_test, y_train, y_test, point_id, n_train = split_train_test(X, y, test_ratio)\n",
    "  # Now we split the data in to different train/cal stes:\n",
    "  x_train, x_test, y_train, y_test, idx_train, idx_cal = preprocess_training_and_scaling(x_train, x_test, y_train, y_test, n_train)\n",
    "\n",
    "  #########################################################\n",
    "  # Neural Network wih a pinball loss\n",
    "  #########################################################\n",
    "\n",
    "  # Convert NumPy arrays to PyTorch tensors\n",
    "  xnn_train = torch.from_numpy(x_train[idx_train]).float()\n",
    "  ynn_train = torch.from_numpy(y_train[idx_train]).float()\n",
    "  xnn_val = torch.from_numpy(x_train[idx_cal]).float()\n",
    "  ynn_val = torch.from_numpy(y_train[idx_cal]).float()\n",
    "  xnn_test = torch.from_numpy(x_test).float()\n",
    "  ynn_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "  # Create DataLoaders for the training and validation data\n",
    "  train_dataset = TensorDataset(xnn_train, ynn_train)\n",
    "  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "  val_dataset = TensorDataset(xnn_val, ynn_val)\n",
    "  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "  # Train models for each quantile with early stopping on validation loss\n",
    "  models = {}\n",
    "  for q in quantiles:\n",
    "    model = QuantileRegression(xnn_train.shape[1], 1)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5  # Number of epochs without improvement to wait before early stopping\n",
    "    early_stopping_count = 0\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "        for data in train_loader:\n",
    "            inputs, targets = data\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(inputs)\n",
    "            loss = pinball_loss(targets, predictions, q)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_targets = val_data\n",
    "                val_predictions = model(val_inputs)\n",
    "                val_loss += pinball_loss(val_targets, val_predictions, q).item()\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_count = 0\n",
    "        else:\n",
    "            early_stopping_count += 1\n",
    "        \n",
    "        if early_stopping_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}.\")\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    models[q] = model\n",
    "\n",
    "  # Make predictions for 0.05 and 0.95 quantiles for the test data\n",
    "  quantile_predictions_test = {q: model(xnn_test).detach().numpy() for q, model in models.items()}\n",
    "\n",
    "  # Extract the lower (0.05) and upper (0.95) quantile predictions\n",
    "  y_lower = quantile_predictions_test[0.05][:, 0]\n",
    "  y_upper = quantile_predictions_test[0.95][:, 0]\n",
    "\n",
    "  output_file = f'test_NN_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "  \n",
    "  #########################################################\n",
    "  # Random forests + Bootstrapping\n",
    "  #########################################################\n",
    "\n",
    "  # Initialize lists to store predictions for each bootstrap sample\n",
    "  bootstrap_predictions = []\n",
    "\n",
    "  # Standard bootstrap: Omit around 36.8% of samples in each iteration\n",
    "  sample_size = int(0.632 * x_train.shape[0])  # 63.2% of training samples\n",
    "    \n",
    "  for _ in range(n_bootstraps):\n",
    "        # Create a new bootstrap sample by randomly selecting samples with replacement\n",
    "        bootstrap_indices = np.random.choice(x_train.shape[0], size=sample_size, replace=False)\n",
    "        X_bootstrap = x_train[bootstrap_indices]\n",
    "        y_bootstrap = y_train[bootstrap_indices]\n",
    "        \n",
    "        # Create and train a new model on the bootstrap sample\n",
    "        bootstrap_model = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf, random_state = 42)\n",
    "        bootstrap_model.fit(X_bootstrap, y_bootstrap)\n",
    "        \n",
    "        # Make predictions on the test data using the bootstrap model\n",
    "        y_bootstrap_pred = bootstrap_model.predict(x_test)\n",
    "        \n",
    "        # Append the predictions to the list\n",
    "        bootstrap_predictions.append(y_bootstrap_pred)\n",
    "    \n",
    "\n",
    "  y_upper = np.max(bootstrap_predictions, axis=0)\n",
    "  y_lower = np.min(bootstrap_predictions, axis=0)\n",
    "\n",
    "  output_file = f'test_Boot_RF_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "\n",
    "  #########################################################\n",
    "  # Quantile random forests + Conformal predicition \n",
    "  #########################################################\n",
    "\n",
    "  model = helper.QuantileForestRegressorAdapter(model = None,\n",
    "                                              fit_params=None,\n",
    "                                              quantiles=quantiles_forest,\n",
    "                                              params = params_qforest)\n",
    "                                              \n",
    "\n",
    "  nc = RegressorNc(model, QuantileRegErrFunc())\n",
    "  y_lower, y_upper = helper.run_icp(nc, x_train, y_train, x_test, idx_train, idx_cal, alpha)\n",
    "  \n",
    "  output_file = f'test_cqr_quantile_forest_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "  \n",
    "  #########################################################\n",
    "  # Prediction Intervals for Gradient Boosting Regression \n",
    "  #########################################################\n",
    "\n",
    "  y_lower, y_upper, point_id = train_gradient_boosting_models(x_train, y_train, x_test, point_id)\n",
    "  output_file = f'test_quantile_GBoost_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)\n",
    "\n",
    "  ########################################################\n",
    "  # Prediction Intervals for Quntile linear\n",
    "  ########################################################\n",
    "\n",
    "  y_lower, y_upper, point_id = train_quantile_linear(x_train, y_train, x_test, point_id)\n",
    "  output_file = f'test_quantile_linear_{i}.csv'\n",
    "  create_and_save_dataframe(y_lower, y_upper, y_test, point_id, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conformal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
